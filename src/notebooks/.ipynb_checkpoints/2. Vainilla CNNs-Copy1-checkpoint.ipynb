{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center><h1>2. Vainilla CNNs</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Necessary modules\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers     import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>dx</th>\n",
       "      <th>dy</th>\n",
       "      <th>tilt_ang</th>\n",
       "      <th>fiducial_yaw_angle</th>\n",
       "      <th>fiducial_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9405933820_9be92ca44f_o.jpg</td>\n",
       "      <td>36</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>f</td>\n",
       "      <td>631</td>\n",
       "      <td>1216</td>\n",
       "      <td>239</td>\n",
       "      <td>239</td>\n",
       "      <td>-15</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9017386@N06</td>\n",
       "      <td>8667610470_4e922b695a_o.jpg</td>\n",
       "      <td>208</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>f</td>\n",
       "      <td>609</td>\n",
       "      <td>324</td>\n",
       "      <td>512</td>\n",
       "      <td>513</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399789333_b862f708bc_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>(25, 32)</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>366</td>\n",
       "      <td>1185</td>\n",
       "      <td>1428</td>\n",
       "      <td>-90</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29671106@N00</td>\n",
       "      <td>11192916406_3288dd6c8f_o.jpg</td>\n",
       "      <td>195</td>\n",
       "      <td>(4, 6)</td>\n",
       "      <td>f</td>\n",
       "      <td>2376</td>\n",
       "      <td>1518</td>\n",
       "      <td>325</td>\n",
       "      <td>325</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9931484274_52c1188826_o.jpg</td>\n",
       "      <td>42</td>\n",
       "      <td>(48, 53)</td>\n",
       "      <td>f</td>\n",
       "      <td>890</td>\n",
       "      <td>623</td>\n",
       "      <td>313</td>\n",
       "      <td>312</td>\n",
       "      <td>-10</td>\n",
       "      <td>30</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image  face_id       age gender     x  \\\n",
       "0  20254529@N04   9405933820_9be92ca44f_o.jpg       36    (4, 6)      f   631   \n",
       "1   9017386@N06   8667610470_4e922b695a_o.jpg      208    (0, 2)      f   609   \n",
       "2  30601258@N03  10399789333_b862f708bc_o.jpg        1  (25, 32)      f     0   \n",
       "3  29671106@N00  11192916406_3288dd6c8f_o.jpg      195    (4, 6)      f  2376   \n",
       "4  20254529@N04   9931484274_52c1188826_o.jpg       42  (48, 53)      f   890   \n",
       "\n",
       "      y    dx    dy  tilt_ang  fiducial_yaw_angle  fiducial_score  \n",
       "0  1216   239   239       -15                   0              23  \n",
       "1   324   512   513        -5                   0              75  \n",
       "2   366  1185  1428       -90                   0              80  \n",
       "3  1518   325   325       200                   0              46  \n",
       "4   623   313   312       -10                  30              79  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the data\n",
    "data = pd.read_csv(filepath_or_buffer=\"../data/face_image_project/fold_0_data.txt\", sep=\"\\t\")\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9405933820_9be92ca44f_o.jpg</td>\n",
       "      <td>36</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9017386@N06</td>\n",
       "      <td>8667610470_4e922b695a_o.jpg</td>\n",
       "      <td>208</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399789333_b862f708bc_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29671106@N00</td>\n",
       "      <td>11192916406_3288dd6c8f_o.jpg</td>\n",
       "      <td>195</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9931484274_52c1188826_o.jpg</td>\n",
       "      <td>42</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image  face_id gender\n",
       "0  20254529@N04   9405933820_9be92ca44f_o.jpg       36      f\n",
       "1   9017386@N06   8667610470_4e922b695a_o.jpg      208      f\n",
       "2  30601258@N03  10399789333_b862f708bc_o.jpg        1      f\n",
       "3  29671106@N00  11192916406_3288dd6c8f_o.jpg      195      f\n",
       "4  20254529@N04   9931484274_52c1188826_o.jpg       42      f"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"user_id\",\"original_image\",\"face_id\",\"gender\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9405933820_9be92ca44f_o.jpg</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9017386@N06</td>\n",
       "      <td>8667610470_4e922b695a_o.jpg</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399789333_b862f708bc_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29671106@N00</td>\n",
       "      <td>11192916406_3288dd6c8f_o.jpg</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9931484274_52c1188826_o.jpg</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image  face_id gender\n",
       "0  20254529@N04   9405933820_9be92ca44f_o.jpg       36      0\n",
       "1   9017386@N06   8667610470_4e922b695a_o.jpg      208      0\n",
       "2  30601258@N03  10399789333_b862f708bc_o.jpg        1      0\n",
       "3  29671106@N00  11192916406_3288dd6c8f_o.jpg      195      0\n",
       "4  20254529@N04   9931484274_52c1188826_o.jpg       42      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.gender != \"m\", \"gender\"] = 0\n",
    "data.loc[data.gender == \"m\", \"gender\"] = 1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>original_image</th>\n",
       "      <th>face_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9405933820_9be92ca44f_o.jpg</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/face_image_project/aligned/20254529@N0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9017386@N06</td>\n",
       "      <td>8667610470_4e922b695a_o.jpg</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/face_image_project/aligned/9017386@N06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30601258@N03</td>\n",
       "      <td>10399789333_b862f708bc_o.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/face_image_project/aligned/30601258@N0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29671106@N00</td>\n",
       "      <td>11192916406_3288dd6c8f_o.jpg</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/face_image_project/aligned/29671106@N0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20254529@N04</td>\n",
       "      <td>9931484274_52c1188826_o.jpg</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>../data/face_image_project/aligned/20254529@N0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                original_image  face_id gender  \\\n",
       "0  20254529@N04   9405933820_9be92ca44f_o.jpg       36      0   \n",
       "1   9017386@N06   8667610470_4e922b695a_o.jpg      208      0   \n",
       "2  30601258@N03  10399789333_b862f708bc_o.jpg        1      0   \n",
       "3  29671106@N00  11192916406_3288dd6c8f_o.jpg      195      0   \n",
       "4  20254529@N04   9931484274_52c1188826_o.jpg       42      0   \n",
       "\n",
       "                                           file_path  \n",
       "0  ../data/face_image_project/aligned/20254529@N0...  \n",
       "1  ../data/face_image_project/aligned/9017386@N06...  \n",
       "2  ../data/face_image_project/aligned/30601258@N0...  \n",
       "3  ../data/face_image_project/aligned/29671106@N0...  \n",
       "4  ../data/face_image_project/aligned/20254529@N0...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_template = \"../data/face_image_project/aligned/%s/landmark_aligned_face.%d.%s\"\n",
    "data[\"file_path\"] = data[[\"user_id\",\"face_id\",\"original_image\"]].apply(lambda x:  \n",
    "                                                                   path_template % (x[0],x[1],x[2]),\n",
    "                                                                   axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/face_image_project/aligned/20254529@N0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/face_image_project/aligned/9017386@N06...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/face_image_project/aligned/30601258@N0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/face_image_project/aligned/29671106@N0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/face_image_project/aligned/20254529@N0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path gender\n",
       "0  ../data/face_image_project/aligned/20254529@N0...      0\n",
       "1  ../data/face_image_project/aligned/9017386@N06...      0\n",
       "2  ../data/face_image_project/aligned/30601258@N0...      0\n",
       "3  ../data/face_image_project/aligned/29671106@N0...      0\n",
       "4  ../data/face_image_project/aligned/20254529@N0...      0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"file_path\",\"gender\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2437\n",
       "1    2047\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Occurences of each gender seem pretty balanced. Looks good to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Y      = data[\"gender\"]\n",
    "X_path = data[\"file_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/face_image_project/aligned/20254529@N04/landmark_aligned_face.36.9405933820_9be92ca44f_o.jpg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    [[[252.0, 242.0, 217.0], [254.0, 244.0, 219.0]...\n",
       "2    [[[0.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0,...\n",
       "Name: file_path, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempo = X_path[1:3]\n",
    "tempo.apply(lambda x: img_to_array( load_img(x) ) ) #DOES IT NEED A RESHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "\n",
    "i_width  = 227\n",
    "i_height = 227\n",
    "\n",
    "#scipy.misc.imresize(original_image, (i_height, i_width))\n",
    "\n",
    "\n",
    "def train_generator(data):\n",
    "    while True:\n",
    "        start, end = 0, 32\n",
    "        while end < len(data):\n",
    "            data = data.sample(frac=1).reset_index(drop=True)\n",
    "            sample  = data[start:end]\n",
    "\n",
    "            X = pd.DataFrame(sample[\"file_path\"].apply(lambda x:  img_to_array( scipy.misc.imresize(load_img(x), (i_height, i_width) ) ) ))\n",
    "            \n",
    "            X = X[\"file_path\"].apply(lambda x: x.reshape((1,)+ x.shape))\n",
    "            X = np.vstack(X)\n",
    "            #print X[0].shape\n",
    "            #print X.shape\n",
    "            #\n",
    "            Y = sample[\"gender\"].as_matrix()\n",
    "            #print Y.as_matrix()\n",
    "            #print X.shape\n",
    "            #print X\n",
    "            yield (X, Y)\n",
    "            start += 32\n",
    "            end += 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Building a vainilla CNN\n",
    "vainilla_cnn = Sequential()\n",
    "vainilla_cnn.add(Convolution2D(32, 3, 3, input_shape=(227,227,3), name='conv1_1'))\n",
    "vainilla_cnn.add(Activation('relu'))\n",
    "vainilla_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "vainilla_cnn.add(Convolution2D(64, 3, 3))\n",
    "vainilla_cnn.add(Activation('relu'))\n",
    "vainilla_cnn.add(Convolution2D(64, 3, 3))\n",
    "vainilla_cnn.add(Activation('relu'))\n",
    "vainilla_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "vainilla_cnn.add(Flatten())\n",
    "vainilla_cnn.add(Dense(output_dim=200, input_dim=500))\n",
    "vainilla_cnn.add(BatchNormalization())\n",
    "vainilla_cnn.add(Activation(\"relu\"))\n",
    "vainilla_cnn.add(Dense(output_dim=100, input_dim=200))\n",
    "vainilla_cnn.add(BatchNormalization())\n",
    "vainilla_cnn.add(Activation(\"relu\"))\n",
    "vainilla_cnn.add(Dense(output_dim=1))\n",
    "vainilla_cnn.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "vainilla_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value conv1_1_W_1\n\t [[Node: conv1_1_W_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1_1_W_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1_1_W_1)]]\n\t [[Node: Mean_9/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1103_Mean_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'conv1_1_W_1/read', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-3cea8b4f6417>\", line 3, in <module>\n    vainilla_cnn.add(Convolution2D(32, 3, 3, input_shape=(227,227,3), name='conv1_1'))\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/models.py\", line 294, in add\n    layer.create_input_layer(batch_input_shape, input_dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/topology.py\", line 398, in create_input_layer\n    self(x)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/topology.py\", line 543, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/layers/convolutional.py\", line 403, in build\n    constraint=self.W_constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/topology.py\", line 415, in add_weight\n    weight = initializer(shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/initializations.py\", line 60, in glorot_uniform\n    return uniform(shape, s, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/initializations.py\", line 33, in uniform\n    return K.random_uniform_variable(shape, -scale, scale, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/backend/tensorflow_backend.py\", line 620, in random_uniform_variable\n    return variable(value, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/backend/tensorflow_backend.py\", line 248, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv1_1_W_1\n\t [[Node: conv1_1_W_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1_1_W_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1_1_W_1)]]\n\t [[Node: Mean_9/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1103_Mean_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9441de7c5ef9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvainilla_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m                                         \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                                         pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1507\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1509\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m                     \u001b[0m_stop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1603\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1604\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value conv1_1_W_1\n\t [[Node: conv1_1_W_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1_1_W_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1_1_W_1)]]\n\t [[Node: Mean_9/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1103_Mean_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'conv1_1_W_1/read', defined at:\n  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-28-3cea8b4f6417>\", line 3, in <module>\n    vainilla_cnn.add(Convolution2D(32, 3, 3, input_shape=(227,227,3), name='conv1_1'))\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/models.py\", line 294, in add\n    layer.create_input_layer(batch_input_shape, input_dtype)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/topology.py\", line 398, in create_input_layer\n    self(x)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/topology.py\", line 543, in __call__\n    self.build(input_shapes[0])\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/layers/convolutional.py\", line 403, in build\n    constraint=self.W_constraint)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/engine/topology.py\", line 415, in add_weight\n    weight = initializer(shape, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/initializations.py\", line 60, in glorot_uniform\n    return uniform(shape, s, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/initializations.py\", line 33, in uniform\n    return K.random_uniform_variable(shape, -scale, scale, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/backend/tensorflow_backend.py\", line 620, in random_uniform_variable\n    return variable(value, dtype=dtype, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/Keras-1.2.0-py2.7.egg/keras/backend/tensorflow_backend.py\", line 248, in variable\n    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 224, in __init__\n    expected_shape=expected_shape)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py\", line 370, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 1424, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value conv1_1_W_1\n\t [[Node: conv1_1_W_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@conv1_1_W_1\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1_1_W_1)]]\n\t [[Node: Mean_9/_51 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_1103_Mean_9\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "vainilla_cnn.fit_generator(train_generator(data), samples_per_epoch=64, nb_epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vainilla_cnn.layers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Define regularizations:\n",
    "def blur_regularization(img, grads, size = (3, 3)):\n",
    "    return cv2.blur(img, size)\n",
    "\n",
    "def decay_regularization(img, grads, decay = 0.8):\n",
    "    return decay * img\n",
    "\n",
    "def clip_weak_pixel_regularization(img, grads, percentile = 1):\n",
    "    clipped = img\n",
    "    threshold = np.percentile(np.abs(img), percentile)\n",
    "    clipped[np.where(np.abs(img) < threshold)] = 0\n",
    "    return clipped\n",
    "\n",
    "def gradient_ascent_iteration(loss_function, img):\n",
    "    loss_value, grads_value = loss_function([img])    \n",
    "    gradient_ascent_step = img + grads_value * 0.9\n",
    "\n",
    "    #Convert to row major format for using opencv routines\n",
    "    grads_row_major = np.transpose(grads_value[0, :], (1, 2, 0))\n",
    "    img_row_major = np.transpose(gradient_ascent_step[0, :], (1, 2, 0))\n",
    "\n",
    "    #List of regularization functions to use\n",
    "    regularizations = [blur_regularization, decay_regularization, clip_weak_pixel_regularization]\n",
    "\n",
    "    #The reguarlization weights\n",
    "    weights = np.float32([3, 3, 1])\n",
    "    weights /= np.sum(weights)\n",
    "\n",
    "    images = [reg_func(img_row_major, grads_row_major) for reg_func in regularizations]\n",
    "    weighted_images = np.float32([w * image for w, image in zip(weights, images)])\n",
    "    img = np.sum(weighted_images, axis = 0)\n",
    "\n",
    "    #Convert image back to 1 x 3 x height x width\n",
    "    img = np.float32([np.transpose(img, (2, 0, 1))])\n",
    "\n",
    "    return img\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "\n",
    "    if x.shape[2] != 3:\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    \n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def get_output_layer(model, layer_name):\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "    layer = layer_dict[layer_name].output\n",
    "    return layer\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def visualize_filter(input_img, filter_index, img_placeholder, number_of_iterations = 20):\n",
    "    layer = vainilla_cnn.layers[0].output #get_output_layer(vainilla_cnn, \"convolution2d_1\")\n",
    "    print layer[:,filter_index,:,:]\n",
    "    loss = K.mean(layer[:, filter_index, :, :])\n",
    "    grads = K.gradients(loss, img_placeholder)[0]\n",
    "    #grads = normalize(grads)\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([img_placeholder], [loss, grads])\n",
    "\n",
    "    img = input_img * 1\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(number_of_iterations):\n",
    "        img = gradient_ascent_iteration(iterate, img)\n",
    "\n",
    "    # decode the resulting input image\n",
    "    img = deprocess_image(img[0])\n",
    "    print \"Done with filter\", filter_index\n",
    "    return img\n",
    "\n",
    "img_width = 227\n",
    "img_height = 227\n",
    "init_img = np.random.random((1, 3, img_width, img_height)) * 20 + 128.\n",
    "\n",
    "layer = vainilla_cnn.layers[0]\n",
    "print layer.output_shape\n",
    "input_placeholder = K.placeholder((1, 3, img_width, img_height))\n",
    "vizualization = visualize_filter(init_img, 0, input_placeholder, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vainilla_cnn.layers[0].input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_width = 227\n",
    "img_height = 227\n",
    "init_img = np.random.random((1, img_width, img_height, 3)) * 20 + 128.\n",
    "\n",
    "get_3rd_layer_output = K.function([vainilla_cnn.layers[0].input],\n",
    "                                  [vainilla_cnn.layers[0].output])\n",
    "layer_output = get_3rd_layer_output([init_img])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "filter0 = layer_output[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "first_layer = vainilla_cnn.layers[-1]\n",
    "# this is a placeholder tensor that will contain our generated images\n",
    "input_img = first_layer.input\n",
    "\n",
    "loss = K.mean(filter0)\n",
    "\n",
    "# compute the gradient of the input picture wrt this loss\n",
    "grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "# normalization trick: we normalize the gradient\n",
    "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
    "\n",
    "# this function returns the loss and grads given the input picture\n",
    "iterate = K.function([input_img], [loss, grads])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
